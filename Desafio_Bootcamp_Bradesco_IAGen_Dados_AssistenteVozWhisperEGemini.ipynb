{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMBOG+wHNrRCStTySATvDX3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarceloJSSantos/bootcamp-bradesco-genai-dados-desadio-assistente-voz-whisper-gemini/blob/main/Desafio_Bootcamp_Bradesco_IAGen_Dados_AssistenteVozWhisperEGemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Desafio do Bootcamp Badesco/DIO GenAI e Dados**\n",
        "### Assistente de Voz com Whisper e Gemini"
      ],
      "metadata": {
        "id": "IumrHSz6Tglt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IDIOMA = 'pt'"
      ],
      "metadata": {
        "id": "xf40IoZFF4PM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Código JavaScript para gravar áudio do usuário\"\n",
        "SCRIPT_JS_RECORD = \"\"\"\n",
        "async function record(silence_duration = 1500) {\n",
        "  // Desativa filtros automáticos que amplificam o ruído de fundo\n",
        "  const stream = await navigator.mediaDevices.getUserMedia({\n",
        "    audio: {\n",
        "      echoCancellation: false,\n",
        "      echoCancellation: false,\n",
        "      noiseSuppression: false,\n",
        "      autoGainControl: false\n",
        "    }\n",
        "  });\n",
        "\n",
        "  const mediaRecorder = new MediaRecorder(stream);\n",
        "  const audioChunks = [];\n",
        "\n",
        "  const audioContext = new AudioContext();\n",
        "  const source = audioContext.createMediaStreamSource(stream);\n",
        "  const analyser = audioContext.createAnalyser();\n",
        "  analyser.fftSize = 512;\n",
        "  source.connect(analyser);\n",
        "\n",
        "  const dataArray = new Uint8Array(analyser.frequencyBinCount);\n",
        "  let silenceStart = performance.now();\n",
        "  let isRecording = true;\n",
        "\n",
        "  // --- CONFIGURAÇÃO DE SENSIBILIDADE ---\n",
        "  // Se o silêncio não for detectado, aumente este valor (ex: para 25 ou 30)\n",
        "  const THRESHOLD = 25;\n",
        "\n",
        "  mediaRecorder.ondataavailable = (event) => audioChunks.push(event.data);\n",
        "\n",
        "  const stopRecording = () => {\n",
        "    return new Promise((resolve) => {\n",
        "      mediaRecorder.onstop = () => {\n",
        "        const audioBlob = new Blob(audioChunks);\n",
        "        const reader = new FileReader();\n",
        "        reader.readAsDataURL(audioBlob);\n",
        "        reader.onloadend = () => resolve(reader.result);\n",
        "      };\n",
        "      mediaRecorder.stop();\n",
        "      stream.getTracks().forEach(track => track.stop());\n",
        "    });\n",
        "  };\n",
        "\n",
        "  mediaRecorder.start();\n",
        "\n",
        "  return new Promise((resolve) => {\n",
        "    const checkAudio = () => {\n",
        "      if (!isRecording) return;\n",
        "\n",
        "      analyser.getByteFrequencyData(dataArray);\n",
        "\n",
        "      // Calcula o volume médio das frequências de voz\n",
        "      let sum = 0;\n",
        "      for (let i = 0; i < dataArray.length; i++) {\n",
        "        sum += dataArray[i];\n",
        "      }\n",
        "      let average = sum / dataArray.length;\n",
        "\n",
        "      // Log para você ver no console do navegador (F12) o nível do seu silêncio\n",
        "      // console.log(\"Volume atual:\", average);\n",
        "\n",
        "      if (average < THRESHOLD) {\n",
        "        if (performance.now() - silenceStart > silence_duration) {\n",
        "          isRecording = false;\n",
        "          resolve(stopRecording());\n",
        "          return;\n",
        "        }\n",
        "      } else {\n",
        "        silenceStart = performance.now(); // Reset se houver som\n",
        "      }\n",
        "\n",
        "      requestAnimationFrame(checkAudio);\n",
        "    };\n",
        "\n",
        "    checkAudio();\n",
        "  });\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "cellView": "form",
        "id": "xjs8env-YaKd"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Gravar Áudio Com Python e JavaScript"
      ],
      "metadata": {
        "id": "_F1n_xqO5w1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-k8Qdfn5c8i"
      },
      "outputs": [],
      "source": [
        "# Referência: https://gist.github.com/korakot/c21c3476c024ad6d56d5f48b0bca92be\n",
        "\n",
        "from IPython.display import Audio, display, Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "\n",
        "import os\n",
        "\n",
        "def gravador(tempo_silencio_sec=2):\n",
        "  # Executa o código JavaScript que captura o audio a ser gravado\n",
        "  display(Javascript(SCRIPT_JS_RECORD))\n",
        "  # Recebe o áudio gravado como resultado do JavaScript\n",
        "  js_result = output.eval_js('record(%s)' % (tempo_silencio_sec * 1000))\n",
        "   # Decodifica o áudio em base64\n",
        "  decode_audio = b64decode(js_result.split(',')[1])\n",
        "  # Define o nome da pasta\n",
        "  pasta = \"audios\"\n",
        "\n",
        "  # Verifica se a pasta já existe antes de criar\n",
        "  if not os.path.exists(pasta):\n",
        "      os.makedirs(pasta)\n",
        "\n",
        "  # Salva o áudio em um arquivo\n",
        "  file_name = f'{pasta}/audio_capturado.mp3'\n",
        "\n",
        "  with open(file_name, 'wb') as f:\n",
        "    f.write(decode_audio)\n",
        "  # Retorna o caminho do arquivo de áudio (pasta padrão do Google Colab)\n",
        "  return f'/content/{file_name}'\n",
        "\n",
        "# Grava o áudio do usuário por um tempo determinado (padrão 5 segundos)\n",
        "print('Faça uma pergunta ao Gemini...\\n')\n",
        "arquivo_audio_entrada = gravador()\n",
        "print(\"Gravação finalizada automaticamente pelo silêncio!\\n\")\n",
        "\n",
        "# Exibe o áudio gravado\n",
        "display(Audio(arquivo_audio_entrada, autoplay=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Reconhecimento de Fala com Whisper (OpenAI)"
      ],
      "metadata": {
        "id": "CA7J_IKO5h1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenta importar a biblioteca, mas cado tenha um erro, pois não está instalado\n",
        "# Instala a biblioteca e então, importa\n",
        "try:\n",
        "    import whisper\n",
        "except ImportError:\n",
        "    print(\"Instalando Whisper...\")\n",
        "    !pip install git+https://github.com/openai/whisper.git -q\n",
        "    import whisper"
      ],
      "metadata": {
        "id": "gKS9ZsML6OY7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecione o modelo do Whisper que melhor atenda às suas necessidades:\n",
        "# https://github.com/openai/whisper#available-models-and-languages\n",
        "model = whisper.load_model(\"small\")\n",
        "\n",
        "# Transcreve o audio gravado anteriormente.\n",
        "result = model.transcribe(arquivo_audio_entrada, fp16=False, language=IDIOMA)\n",
        "transcricao_whisper = result[\"text\"]\n",
        "print(transcricao_whisper)"
      ],
      "metadata": {
        "id": "563soyV460-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Integração com a API do Gemini (por ser gratuito)"
      ],
      "metadata": {
        "id": "QcnuZxCLFe3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenta importar a biblioteca, mas cado tenha um erro, pois não está instalado\n",
        "# Instala a biblioteca e então, importa\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "except ImportError:\n",
        "    print(\"Instalando genai...\")\n",
        "    !pip install -q -U google-generativeai\n",
        "    import google.generativeai as genai"
      ],
      "metadata": {
        "id": "mBXygNnFFjel"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Substitua pela sua chave real do AI Studio\n",
        "minha_chave = \"COLOQUE SUA CHAVE GEMINI\"\n",
        "genai.configure(api_key=minha_chave)\n",
        "\n",
        "# 1. Usando o modelo mais atual (2.0 Flash é o padrão agora)\n",
        "# Se der erro, você pode tentar 'gemini-1.5-flash' sem o v1beta no nome\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "\n",
        "try:\n",
        "    # 2. Gerando o conteúdo\n",
        "    resposta_gemini = model.generate_content(transcricao_whisper)\n",
        "    resposta_gemini_texto = resposta_gemini.text.replace(\"*\", \"\")\n",
        "\n",
        "    # 3. Exibindo o texto\n",
        "    print(\"Resposta do Gemini:\")\n",
        "    print(resposta_gemini_texto)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Ocorreu um erro: {e}\")\n",
        "    # Caso o modelo 2.0 ainda não esteja na sua região, listamos os disponíveis:\n",
        "    print(\"\\nModelos que você pode usar:\")\n",
        "    for m in genai.list_models():\n",
        "        if 'generateContent' in m.supported_generation_methods:\n",
        "            print(m.name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "79_5p-SDGS3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Transformando a Resposta do Gemini em Voz com edge_tts (versão da voz gratuita + natural)"
      ],
      "metadata": {
        "id": "n4XDOQ2T0-jL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenta importar a biblioteca, mas cado tenha um erro, pois não está instalado\n",
        "# Instala a biblioteca e então, importa\n",
        "try:\n",
        "    import edge_tts\n",
        "    import asyncio\n",
        "except ImportError:\n",
        "    print(\"Instalando edge-tts...\")\n",
        "    !pip install edge-tts\n",
        "    import edge_tts\n",
        "    import asyncio"
      ],
      "metadata": {
        "id": "ARdm4D388akI"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_audio = '/content/audios/audio_sintetizado.mp3'\n",
        "\n",
        "async def sintetizar():\n",
        "    VOZ = \"pt-BR-FranciscaNeural\" # Ou \"pt-BR-AntonioNeural\"\n",
        "\n",
        "    communicate = edge_tts.Communicate(resposta_gemini_texto, VOZ)\n",
        "\n",
        "    await communicate.save(response_audio)\n",
        "\n",
        "# No Colab, usamos o loop de eventos atual\n",
        "await sintetizar()\n",
        "\n",
        "# Para ouvir no Colab:\n",
        "from IPython.display import Audio\n",
        "Audio(response_audio, autoplay=True)"
      ],
      "metadata": {
        "id": "UaixY9ew802S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}